Question1.1a

Question1.1b

Question1.2a

Question1.2b

Question2.1

Question2.2a

Question2.2b

Question2.2c
Using L2 regularization and dropout on the dense layers was wrong according to piazza. Should be:
- dropout after activation of the hidden layers
- L2 regularization used as weight_decay param in SGD optimizer ("at optimizer level")

The latter shows that dropout causes more oscillations in the training accuracy and losses, 
probably due to randomly switching on/off certain neurons. L2 regularization shows more stable losses and accuracies due to
the magntiude of weights being minimized. i.e. differences between networks are smaller when using L2-regularization...?

Question3.1a

Question3.1b

Question3.1c